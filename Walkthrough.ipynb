{"cells":[{"cell_type":"markdown","metadata":{},"source":["TEXT CLASSIFICATION USING SVM"]},{"cell_type":"markdown","metadata":{},"source":["INTRODUCTION - \n","In this exercies I was tasked with creating code that can clean a raw dataset and extract techinical skills.  I determined that a one-class classification algorithm would be suitable for this binary classification task. With a severely skewed example class distribution, this technique allows me to fit on the input examples from the majority class in the training dataset, then evaluate on a  test dataset with both classes.\n","\n","I am using a one-class SVM with a non-linear kernel. One-class SVM is perfect for this problem since it's an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. I am utilizing the example technical skills as my training dataset. This dataset has contains 979 instances of a negative case (class -1), which is a normal case, and 0 instances of a postive case (class 1), which is an outlier. "]},{"cell_type":"code","execution_count":333,"metadata":{"id":"LjhiUFpKcfKO"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import gensim\n","import re\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from collections import Iterable\n"]},{"cell_type":"markdown","metadata":{},"source":["Loading the Data"]},{"cell_type":"code","execution_count":228,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                       Technology Skills\n","0                    SAP Fiori Developer\n","1  Oracle Instance Management & Strategy\n","2           Boomi Master Data Management\n","3  Digital Manufacturing on Cloud ( DMC)\n","4                                 DevOps\n"]}],"source":["train_df = pd.read_csv('Example_Technical_Skills.csv')\n","print(train_df.head())"]},{"cell_type":"code","execution_count":231,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["          RAW DATA\n","0         What ifs\n","1        seniority\n","2      familiarity\n","3  functionalities\n","4          Lambdas\n","0           What ifs\n","1          seniority\n","2        familiarity\n","3    functionalities\n","4            Lambdas\n","Name: RAW DATA, dtype: object\n"]}],"source":["test_df = pd.read_csv('Raw_Skills_Dataset.csv')\n","print(test_df.head())\n","X = test_df['RAW DATA']\n","print(X.head())"]},{"cell_type":"markdown","metadata":{},"source":["In this section I am going to train word embeddings using the data and in order to train those word vectors I'll tokenize the data."]},{"cell_type":"code","execution_count":271,"metadata":{},"outputs":[{"data":{"text/plain":["['SAP', 'Fiori', 'Developer']"]},"execution_count":271,"metadata":{},"output_type":"execute_result"}],"source":["x = train_df['Technology Skills']\n","x_train_tokenized = [[w for w in sentence.split(\" \")] for sentence in x]\n","x_train_tokenized[0]"]},{"cell_type":"code","execution_count":342,"metadata":{},"outputs":[],"source":["#clean the test data before tokenization\n","def clean_text(text):\n","    cleaned = re.sub('[^A-Za-z0-9]+', '', text)\n","    return cleaned.strip()\n","\n","x_cleaned = [clean_text(t) for t in X]"]},{"cell_type":"code","execution_count":233,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['What', 'ifs']\n"]}],"source":["x_test_tokenzied = [[w for w in sentence.split(\" \")] for sentence in X_cleaned]\n","print(x_test_tokenzied[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["Create word embeddings model"]},{"cell_type":"code","execution_count":234,"metadata":{},"outputs":[],"source":["model = gensim.models.Word2Vec(x_train_tokenized,\n","                 size=100, \n","                 window=5, \n","                 min_count=1, \n","                 workers=2, \n","                 sg=1\n","                )\n"]},{"cell_type":"markdown","metadata":{},"source":["I will create a class called Sequencer that will convert texts into word embedding sequences.\n","The constructor function for the class takes 4 parameters: all_words, max_words, seq_length, embedding_matrix\n","All Words = The entire dataset will be giben in a list format which contains all tokens\n","Max Words = This parameter will be used in finding most used word\n","Sequence Length = In machine learning our dataset's number of variable has to be specified. But in real life each sentence might has a different length. In order to prevent this problem I determiend a length of 3 and will adapt each sentence to that length.\n","Embedding Matrix = Alist of all words and their corresponding embeddings"]},{"cell_type":"code","execution_count":343,"metadata":{},"outputs":[],"source":["class Sequencer():\n","    \n","    def __init__(self,\n","                 all_words,\n","                 max_words,\n","                 seq_len,\n","                 embedding_matrix\n","                ):\n","        \n","        self.seq_len = seq_len\n","        self.embed_matrix = embedding_matrix\n","        \"\"\"\n","        temp_vocab = Vocab which has all the unique words\n","        self.vocab = The last vocab which has only most used N words.\n","    \n","        \"\"\"\n","        temp_vocab = list(set(all_words))\n","        self.vocab = []\n","        self.word_counts = {}\n","        \"\"\"\n","        Now I'll create a hash map (dict) which includes words and their occurencies\n","        \"\"\"\n","        for word in temp_vocab:\n","            count = len([0 for w in all_words if w == word])\n","            self.word_counts[word] = count\n","            counts = list(self.word_counts.values())\n","            indexes = list(range(len(counts)))\n","        \n","        # Now I'll sort counts and while sorting them also will sort indexes.\n","        # I'll use those indexes to find most used N word.\n","        cnt = 0\n","        while cnt + 1 != len(counts):\n","            cnt = 0\n","            for i in range(len(counts)-1):\n","                if counts[i] < counts[i+1]:\n","                    counts[i+1],counts[i] = counts[i],counts[i+1]\n","                    indexes[i],indexes[i+1] = indexes[i+1],indexes[i]\n","                else:\n","                    cnt += 1\n","        \n","        for ind in indexes[:max_words]:\n","            self.vocab.append(temp_vocab[ind])\n","                    \n","    def textToVector(self,text):\n","        # First I need to split the text into its tokens and learn the length\n","        # If length is shorter than the max len I'll add some spaces (100D vectors which has only zero values)\n","        # If it's longer than the max len I'll trim from the end.\n","        tokens = text.split()\n","        len_v = len(tokens)-1 if len(tokens) < self.seq_len else self.seq_len-1\n","        vec = []\n","        for tok in tokens[:len_v]:\n","            try:\n","                vec.append(self.embed_matrix[tok])\n","            except Exception as E:\n","                pass\n","        \n","        last_pieces = self.seq_len - len(vec)\n","        for i in range(last_pieces):\n","            vec.append(np.zeros(100,))\n","        \n","        return np.asarray(vec).flatten()\n","                \n","                "]},{"cell_type":"code","execution_count":236,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1368\n"]}],"source":["max_words = len(model.wv.vocab.items())\n","print(max_words)"]},{"cell_type":"code","execution_count":239,"metadata":{},"outputs":[],"source":["\n","sequencer = Sequencer(all_words = [token for seq in x_train_tokenized for token in seq],\n","              max_words = max_words,\n","              seq_len = 3,\n","              embedding_matrix = model.wv\n","             )"]},{"cell_type":"code","execution_count":240,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[-6.77003118e-04 -1.66989898e-03  2.44807033e-03 -1.60491443e-03\n"," -2.95602274e-03 -1.14822015e-03  1.45337312e-03  3.70449293e-03\n","  4.82298387e-03  3.43484618e-03 -5.66118630e-04 -2.81287055e-03\n","  3.25641548e-03 -3.14594107e-03 -2.00625625e-03 -4.20035329e-03\n"," -2.91205989e-03 -1.53663137e-03 -9.77550400e-04  2.19090306e-03\n"," -3.65276216e-03 -7.33121560e-05  3.48160742e-04 -4.73049656e-03\n"," -7.70643936e-04 -2.83700996e-03  2.50594970e-03  1.52498495e-03\n","  1.77432562e-03  4.62842034e-03 -4.55416134e-03  1.38183811e-03\n","  2.07295598e-04 -2.17316533e-03  1.36961858e-03 -7.02824327e-04\n"," -3.83880269e-03 -1.48214749e-03 -4.08658572e-03  2.35800678e-03\n"," -3.31553956e-03  2.48296955e-03  1.70158106e-03 -4.86065540e-03\n"," -1.57628674e-03  3.37599218e-03  4.66610305e-03  3.46359308e-03\n"," -5.66189177e-04 -6.85683335e-04  3.20699095e-04  1.35756889e-03\n","  3.87308840e-03  3.01182014e-03 -3.46286013e-03 -4.95317951e-03\n","  1.71005714e-03 -1.87465281e-03  2.77475570e-03  4.25478024e-03\n"," -2.31088814e-03  7.82835588e-04  1.52426818e-03 -3.79062840e-03\n"," -4.58124792e-03 -3.51591199e-03 -3.53277475e-03 -2.66421633e-03\n","  3.02794995e-03  1.81887124e-03 -4.12404304e-03 -4.38242778e-03\n"," -2.58313003e-03  1.08428416e-03  4.74438071e-04  3.62120755e-03\n","  4.39586304e-03  3.78223672e-03 -4.07298189e-03  2.69341841e-03\n","  3.22523457e-03 -2.04912084e-03  1.00451615e-03  1.87302823e-03\n","  4.50402498e-03 -1.51279243e-03  1.52928056e-03 -5.46028430e-04\n"," -2.79041240e-03 -2.50964961e-03 -7.25227728e-05 -4.89698024e-03\n","  2.35405937e-03  3.06986156e-03 -4.91545303e-04  2.23703682e-04\n"," -4.05343063e-03 -4.21881668e-05 -1.64001773e-03  3.22805461e-03\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n","  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n","(300,)\n"]}],"source":["test_vec = sequencer.textToVector(\"Natural Language Processing\")\n","print(test_vec)\n","print(test_vec.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Everything looks fine, but as you see each vector for a sentence has 300 elements and it'll consume a lot of time to train a Support Vector Machine Classifier on this.\n","\n","In order to prevent a long run time I am going to reduce the the dimensionality of the dataset utilizing Principal Component Analysis. With PCA, I can find a balance between creating a reduced set of variables and capturing a large percentager of the variation."]},{"cell_type":"code","execution_count":241,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(979, 300)\n","(34116, 300)\n"]}],"source":["# But before creating a PCA model using scikit-learn let's create\n","# vectors for our each vector\n","x_train_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_train_tokenized])\n","print(x_train_vecs.shape)\n","x_test_vecs = np.asarray([sequencer.textToVector(\" \".join(seq)) for seq in x_test_tokenzied])\n","print(x_test_vecs.shape)"]},{"cell_type":"code","execution_count":242,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Sum of variance ratios:  0.9595982205442048\n"]}],"source":["pca_model = PCA(n_components=150)\n","pca_model.fit(x_train_vecs)\n","print(\"Sum of variance ratios: \",sum(pca_model.explained_variance_ratio_))"]},{"cell_type":"markdown","metadata":{},"source":["Utilizing 150 principal components I can capture roughly 96% of the data's variance while cutting the the dimensionality of the dataset in half.\n","This result is satisfactory, so I am going to use the transform function and reduce the dimensionionality."]},{"cell_type":"code","execution_count":243,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(979, 150)\n","(34116, 150)\n"]}],"source":["x_train_comps = pca_model.transform(x_train_vecs)\n","print(x_train_comps.shape)\n","x_test_comps = pca_model.transform(x_test_vecs)\n","print(x_test_comps.shape)"]},{"cell_type":"markdown","metadata":{},"source":["After conducting PCA, the model is ready to be created. I am using a one-class SVM with a non-linear kernel.\n"]},{"cell_type":"code","execution_count":347,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]}],"source":["clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.0000087)\n","clf.fit(x_train_comps)\n","print(clf.fit_status_)\n"]},{"cell_type":"markdown","metadata":{},"source":["A fit status of zero indicates that the model is correctly fitted onto the training data, so now it is time to genereate predictions on the test data."]},{"cell_type":"code","execution_count":348,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 1  1  1 ... -1  1  1]\n"]}],"source":["predictions = clf.predict(x_test_comps)\n","print(predictions)"]},{"cell_type":"code","execution_count":349,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(array([    5,     6,    35, ..., 34103, 34110, 34113], dtype=int64),)\n","(array([    0,     1,     2, ..., 34112, 34114, 34115], dtype=int64),)\n"]}],"source":["x_normal_array = np.where(predictions == -1)\n","x_outlier_array = np.where(predictions == 1)\n","print(x_normal_array)\n","print(x_outlier_array)\n"]},{"cell_type":"markdown","metadata":{},"source":["After seperating the predictions from my model I have to find the indeces for every prediction."]},{"cell_type":"code","execution_count":322,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[5, 6, 35, 42, 64, 69, 73, 74, 79, 94]\n","[0, 1, 2, 3, 4, 7, 8, 9, 10, 11]\n"]}],"source":["x_normal_inds = np.array(x_normal_array).tolist()\n","x_outlier_inds = np.array(x_outlier_array).tolist()\n","x_normal_inds = [num for sublist in x_normal_inds for num in sublist]\n","print(x_normal_inds[:10])\n","x_outlier_inds = [num for sublist in x_outlier_inds for num in sublist]\n","print(x_outlier_inds[:10])"]},{"cell_type":"markdown","metadata":{},"source":["Now that I have my indices ready I can access the original text and create new lists that have the technical skills and jargon separated."]},{"cell_type":"code","execution_count":323,"metadata":{},"outputs":[],"source":["technical_skills = []\n","jargon = []\n","for i in x_normal_inds:\n","    technical_skills.append(X[i])\n","for i in x_outlier_inds:\n","    jargon.append(X[i])"]},{"cell_type":"markdown","metadata":{},"source":["Finally, I convert the list into csv files for my submission."]},{"cell_type":"code","execution_count":324,"metadata":{},"outputs":[],"source":["tc = pd.DataFrame(technical_skills)\n","j = pd.DataFrame(jargon)\n","tc.to_csv('Cleaned_Skills.csv')\n","j.to_csv('Jargon.csv')\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNy7HmYzVOueIve5e8Zef2j","name":"Text.ipynb","provenance":[]},"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"},"kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
